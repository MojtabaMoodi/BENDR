{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Fine-tuning\n",
    "-----------\n",
    "A common paradigm in deep learning is to pretrain a model on relevant data and then *fine-tune* it on smaller\n",
    "application-focused examples.\n",
    "\n",
    "We show how this premise can be used from the perspective of a large dataset of many people, and see each\n",
    "person as a fine-tuning opportunity. This is very-similar to the un-aligned/DA/DG case of fine-tuning from\n",
    "Kostas and Rudzicz 2020 (https://doi.org/10.1088/1741-2552/abb7a7).\n",
    "\n",
    "To keep things as simple as possible, we use pretty much the same configuration and, as much as possible, code\n",
    "as the `Basics` example. Return to that if anything is confusing.\n",
    "\n",
    "```yaml\n",
    "Configuratron:\n",
    "  preload: True\n",
    "\n",
    "use_gpu: False\n",
    "test_fraction: 0.5\n",
    "\n",
    "mmidb:\n",
    "  name: \"Physionet MMIDB\"\n",
    "  toplevel: /path/to/eegmmidb\n",
    "  tmin: 0\n",
    "  tlen: 6\n",
    "  data_max: 0.001\n",
    "  data_min: -0.001\n",
    "  events:\n",
    "    - T1\n",
    "    - T2\n",
    "  exclude_sessions:\n",
    "    - \"*R0[!48].edf\"  # equivalently \"*R0[1235679].edf\"\n",
    "    - \"*R1[!2].edf\"   # equivalently \"*R1[134].edf\"\n",
    "  exclude_people:\n",
    "    - S088\n",
    "    - S090\n",
    "    - S092\n",
    "    - S100\n",
    "  train_params:\n",
    "    epochs: 7\n",
    "    batch_size: 4\n",
    "  lr: 0.0001\n",
    "  fine_lr: 0.00001\n",
    "  folds: 5\n",
    "```\n",
    "\n",
    "Below we will start with some identical code to load our dataset, and prepare a TIDNet model for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding additional configuration entries: dict_keys(['train_params', 'folds', 'lr'])\n",
      "Configuratron found 1 datasets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning /Volumes/Data/MMI. If there are a lot of files, this may take a while...: 100%|██████████| 4/4 [00:03<00:00,  1.05it/s, extension=.gdf]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset of 315 Preloaded Epoched recordings from 105 people.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Physionet MMIDB: 100%|██████████| 105/105 [00:10<00:00,  9.88person/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Physionet MMIDB | DSID: None | 105 people | 4408 trials | 90 channels | 1536 samples/trial | 256Hz | 0 transforms\n",
      "Constructed 1 channel maps\n",
      "====================\n",
      "Used by 315 recordings:\n",
      "EEG (original(new)): Fc5.(FC5) Fc3.(FC3) Fc1.(FC1) Fcz.(FCZ) Fc2.(FC2) Fc4.(FC4) Fc6.(FC6) C5..(C5) C3..(C3) C1..(C1) Cz..(CZ) C2..(C2) C4..(C4) C6..(C6) Cp5.(CP5) Cp3.(CP3) Cp1.(CP1) Cpz.(CPZ) Cp2.(CP2) Cp4.(CP4) Cp6.(CP6) Fp1.(FP1) Fpz.(FPZ) Fp2.(FP2) Af7.(AF7) Af3.(AF3) Afz.(AFZ) Af4.(AF4) Af8.(AF8) F7..(F7) F5..(F5) F3..(F3) F1..(F1) Fz..(FZ) F2..(F2) F4..(F4) F6..(F6) F8..(F8) Ft7.(FT7) Ft8.(FT8) T7..(T7) T8..(T8) T9..(T9) T10.(T10) Tp7.(TP7) Tp8.(TP8) P7..(P7) P5..(P5) P3..(P3) P1..(P1) Pz..(PZ) P2..(P2) P4..(P4) P6..(P6) P8..(P8) Po7.(PO7) Po3.(PO3) Poz.(POZ) Po4.(PO4) Po8.(PO8) O1..(O1) Oz..(OZ) O2..(O2) Iz..(IZ) \n",
      "EOG (original(new)): \n",
      "REF (original(new)): \n",
      "EXTRA (original(new)): \n",
      "Heuristically Assigned: Fc5.(FC5)  Fc3.(FC3)  Fc1.(FC1)  Fcz.(FCZ)  Fc2.(FC2)  Fc4.(FC4)  Fc6.(FC6)  C5..(C5)  C3..(C3)  C1..(C1)  Cz..(CZ)  C2..(C2)  C4..(C4)  C6..(C6)  Cp5.(CP5)  Cp3.(CP3)  Cp1.(CP1)  Cpz.(CPZ)  Cp2.(CP2)  Cp4.(CP4)  Cp6.(CP6)  Fp1.(FP1)  Fpz.(FPZ)  Fp2.(FP2)  Af7.(AF7)  Af3.(AF3)  Afz.(AFZ)  Af4.(AF4)  Af8.(AF8)  F7..(F7)  F5..(F5)  F3..(F3)  F1..(F1)  Fz..(FZ)  F2..(F2)  F4..(F4)  F6..(F6)  F8..(F8)  Ft7.(FT7)  Ft8.(FT8)  T7..(T7)  T8..(T8)  T9..(T9)  T10.(T10)  Tp7.(TP7)  Tp8.(TP8)  P7..(P7)  P5..(P5)  P3..(P3)  P1..(P1)  Pz..(PZ)  P2..(P2)  P4..(P4)  P6..(P6)  P8..(P8)  Po7.(PO7)  Po3.(PO3)  Poz.(POZ)  Po4.(PO4)  Po8.(PO8)  O1..(O1)  Oz..(OZ)  O2..(O2)  Iz..(IZ) \n",
      "--------------------\n",
      "Excluded []\n",
      "====================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from dn3.configuratron import ExperimentConfig\n",
    "from dn3.trainable.processes import StandardClassification\n",
    "from dn3.trainable.models import TIDNet\n",
    "\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# Since we are doing a lot of loading, this is nice to suppress some tedious information\n",
    "import mne\n",
    "mne.set_log_level(False)\n",
    "\n",
    "config_filename = 'my_config.yml'\n",
    "experiment = ExperimentConfig(config_filename)\n",
    "ds_config = experiment.datasets['mmidb']\n",
    "\n",
    "dataset = ds_config.auto_construct_dataset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This time, we will also create two functions that exhibit the two different (though not necessarily mutually exclusive)\n",
    "way one might adjust from one domain to a slightly different one. Freezing and fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def frozen_tuning(training_data, testing_data, model):\n",
    "    model.freeze_features()\n",
    "    tune_process = StandardClassification(model, learning_rate=ds_config.rate, cuda=experiment.use_gpu)\n",
    "    tune_process.fit(training_data, **ds_config.train_params)\n",
    "    # We unfreeze so that the model can be subsequently trained again\n",
    "    model.freeze_features(unfreeze=True)\n",
    "    return tune_process.evaluate(testing_data)['Accuracy']\n",
    "\n",
    "def fine_tuning(training_data, testing_data, model):\n",
    "    tune_process = StandardClassification(model, learning_rate=ds_config.fine_lr, cuda=experiment.use_gpu,)\n",
    "    tune_process.fit(training_data, **ds_config.train_params)\n",
    "    return tune_process.evaluate(testing_data)['Accuracy']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we'll make some helpers to compare the tuned performance for three possible scenarios:\n",
    "\n",
    "    1. Freeze features with a new classifier\n",
    "    2. The same as the above, but then fine-tune *all weights* including the new final layer\n",
    "    3. Just fine-tuning all the general weights from the start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "PERFORMANCE_COLUMNS = ['Tuned', 'Frozen', 'Frozen then Tuned']\n",
    "def tuning_performance_comparison(training_data, testing_data, model):\n",
    "    just_tune_model = model.clone()\n",
    "    just_tune_performance = fine_tuning(training_data, testing_data, just_tune_model)\n",
    "\n",
    "    freeze_performance = frozen_tuning(training_data, testing_data, model)\n",
    "    freeze_then_tune = fine_tuning(training_data, testing_data, model)\n",
    "\n",
    "    return dict(zip(PERFORMANCE_COLUMNS, (just_tune_performance, freeze_performance, freeze_then_tune)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Now everything runs pretty much the same as our basic process, with person-specific performance reporting. Except this\n",
    "time, we will compare the different tuning techniques instead of just evaluating the model with our test person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LMSO:   0%|          | 0/5 [00:00<?, ?fold/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:   >> Physionet MMIDB | DSID: None | 63 people | 2646 trials | 90 channels | 1536 samples/trial | 256Hz | 0 transforms\n",
      "Validation: >> Physionet MMIDB | DSID: None | 21 people | 880 trials | 90 channels | 1536 samples/trial | 256Hz | 0 transforms\n",
      "Test:       >> Physionet MMIDB | DSID: None | 21 people | 882 trials | 90 channels | 1536 samples/trial | 256Hz | 0 transforms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mojtaba/Library/CloudStorage/OneDrive-UniversityofWaterloo/Think/Time Management/UWaterloo/Research/my research/BENDR/env/lib/python3.12/site-packages/torch/nn/utils/weight_norm.py:143: FutureWarning: `torch.nn.utils.weight_norm` is deprecated in favor of `torch.nn.utils.parametrizations.weight_norm`.\n",
      "  WeightNorm.apply(module, name, dim)\n",
      "LMSO:   0%|          | 0/5 [00:00<?, ?fold/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TIDNet using: 90 channels x 1536 samples at 256Hz | 2 targets\n",
      "Apple M-series GPU detected: training and model execution will be performed on MPS.\n",
      "General training...\n",
      "Loading data with 0 additional workers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fadeecfb6d774ba099a4b039cdb32ccc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:  14%|#4        | 1/7 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c648ade9ba264f0797391334b29e68a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 1/661 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LMSO:   0%|          | 0/5 [01:03<?, ?fold/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: End of Epoch 1 | Accuracy: 63.09% | loss: 0.696 | lr: 3.539e-05 | momentum: 0.917 | epoch: 1.000 |\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fad7cea266b04d3a99c22c2d6980286d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting:   0%|          | 0/220 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LMSO:   0%|          | 0/5 [01:10<?, ?fold/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: End of Epoch 1 | Accuracy: 78.75% | loss: 0.444 |\n",
      "Best loss. Retaining checkpoint...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "155a19aa95cb4fd8bc498d6c0edb32c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 1/661 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LMSO:   0%|          | 0/5 [02:13<?, ?fold/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: End of Epoch 2 | Accuracy: 82.07% | loss: 0.394 | lr: 9.699e-05 | momentum: 0.853 | epoch: 2.000 |\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d47d9fb9c554de8ba8f95884fc8261b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting:   0%|          | 0/220 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LMSO:   0%|          | 0/5 [02:21<?, ?fold/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: End of Epoch 2 | Accuracy: 81.25% | loss: 0.379 |\n",
      "Best loss. Retaining checkpoint...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "078f7563202c4993919133a0ba1ba1e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 1/661 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LMSO:   0%|          | 0/5 [03:25<?, ?fold/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: End of Epoch 3 | Accuracy: 85.14% | loss: 0.342 | lr: 9.021e-05 | momentum: 0.860 | epoch: 3.000 |\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20719fe392644709b56150294c648d7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting:   0%|          | 0/220 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LMSO:   0%|          | 0/5 [03:33<?, ?fold/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: End of Epoch 3 | Accuracy: 83.52% | loss: 0.353 |\n",
      "Best loss. Retaining checkpoint...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e878ee71f1ce4340949e1883695e357f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 1/661 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LMSO:   0%|          | 0/5 [04:41<?, ?fold/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: End of Epoch 4 | Accuracy: 86.91% | loss: 0.315 | lr: 6.883e-05 | momentum: 0.881 | epoch: 4.000 |\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f383302e0fe742c099d09e8922f6414b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting:   0%|          | 0/220 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LMSO:   0%|          | 0/5 [04:48<?, ?fold/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: End of Epoch 4 | Accuracy: 83.98% | loss: 0.348 |\n",
      "Best loss. Retaining checkpoint...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7e25e20ba9b4a748c2bf8ddc46b4d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 1/661 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LMSO:   0%|          | 0/5 [05:55<?, ?fold/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: End of Epoch 5 | Accuracy: 87.93% | loss: 0.293 | lr: 4.167e-05 | momentum: 0.908 | epoch: 5.000 |\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea78df9fe887459c95d91878d42ade30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting:   0%|          | 0/220 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LMSO:   0%|          | 0/5 [06:02<?, ?fold/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: End of Epoch 5 | Accuracy: 85.00% | loss: 0.334 |\n",
      "Best loss. Retaining checkpoint...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f08fd78711b4564a8294fe885a2b3a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 1/661 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LMSO:   0%|          | 0/5 [07:03<?, ?fold/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: End of Epoch 6 | Accuracy: 89.71% | loss: 0.253 | lr: 1.707e-05 | momentum: 0.933 | epoch: 6.000 |\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c945ff2b55a04d8e82562e96b02af17b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting:   0%|          | 0/220 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LMSO:   0%|          | 0/5 [07:10<?, ?fold/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: End of Epoch 6 | Accuracy: 85.23% | loss: 0.334 |\n",
      "Best loss. Retaining checkpoint...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71d249fe977f4ec5859bce1a8f7731e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Iteration:   0%|          | 1/661 [00:00<?, ?batches/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LMSO:   0%|          | 0/5 [08:11<?, ?fold/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: End of Epoch 7 | Accuracy: 90.13% | loss: 0.240 | lr: 2.565e-06 | momentum: 0.947 | epoch: 7.000 |\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84080c919e3749599188fd7484633c24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting:   0%|          | 0/220 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LMSO:   0%|          | 0/5 [08:18<?, ?fold/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation: End of Epoch 7 | Accuracy: 85.23% | loss: 0.333 |\n",
      "Best loss. Retaining checkpoint...\n",
      "Loading best model...\n",
      "Fine tuning...\n",
      "Training:   >> Physionet MMIDB | DSID: None | 19 people | 798 trials | 90 channels | 1536 samples/trial | 256Hz | 0 transforms\n",
      "Validation: Person S021 - 42 trials | 0 transforms\n",
      "Test:       Person S001 - 42 trials | 0 transforms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DatasetConfig' object has no attribute 'fine_lr'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, _, test_thinker \u001b[38;5;129;01min\u001b[39;00m test.loso():\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# Now split the test_thinker further for training and testing (the middle return value would be validation)\u001b[39;00m\n\u001b[32m     16\u001b[39m     tune_train, _, tune_test = test_thinker.split(test_frac=experiment.test_fraction, validation_frac=\u001b[32m0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     performance = \u001b[43mtuning_performance_comparison\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtune_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtune_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtidnet\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m     best_perf = \u001b[38;5;28mmax\u001b[39m(performance.values())\n\u001b[32m     20\u001b[39m     tqdm.tqdm.write(\u001b[33m\"\u001b[39m\u001b[33mEvaluated person \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m, Best performance: \u001b[39m\u001b[38;5;132;01m{:.2%}\u001b[39;00m\u001b[33m\"\u001b[39m.format(test_thinker.person_id, best_perf))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mtuning_performance_comparison\u001b[39m\u001b[34m(training_data, testing_data, model)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtuning_performance_comparison\u001b[39m(training_data, testing_data, model):\n\u001b[32m      3\u001b[39m     just_tune_model = model.clone()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m     just_tune_performance = \u001b[43mfine_tuning\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtesting_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjust_tune_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m     freeze_performance = frozen_tuning(training_data, testing_data, model)\n\u001b[32m      7\u001b[39m     freeze_then_tune = fine_tuning(training_data, testing_data, model)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36mfine_tuning\u001b[39m\u001b[34m(training_data, testing_data, model)\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfine_tuning\u001b[39m(training_data, testing_data, model):\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     tune_process = StandardClassification(model, learning_rate=\u001b[43mds_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfine_lr\u001b[49m, cuda=experiment.use_gpu,)\n\u001b[32m     11\u001b[39m     tune_process.fit(training_data, **ds_config.train_params)\n\u001b[32m     12\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tune_process.evaluate(testing_data)[\u001b[33m'\u001b[39m\u001b[33mAccuracy\u001b[39m\u001b[33m'\u001b[39m]\n",
      "\u001b[31mAttributeError\u001b[39m: 'DatasetConfig' object has no attribute 'fine_lr'"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "results = []\n",
    "for training, validation, test in tqdm.tqdm(dataset.lmso(ds_config.folds), total=ds_config.folds,\n",
    "                                            desc=\"LMSO\", unit='fold'):\n",
    "    tidnet = TIDNet.from_dataset(dataset)\n",
    "    general_process = StandardClassification(tidnet, cuda=experiment.use_gpu, learning_rate=ds_config.lr)\n",
    "\n",
    "    # General training\n",
    "    tqdm.tqdm.write(\"General training...\")\n",
    "    general_process.fit(training_dataset=training, validation_dataset=validation, **ds_config.train_params)\n",
    "\n",
    "    # Tuning\n",
    "    tqdm.tqdm.write(\"Fine tuning...\")\n",
    "    for _, _, test_thinker in test.loso():\n",
    "        # Now split the test_thinker further for training and testing (the middle return value would be validation)\n",
    "        tune_train, _, tune_test = test_thinker.split(test_frac=experiment.test_fraction, validation_frac=0)\n",
    "\n",
    "        performance = tuning_performance_comparison(tune_train, tune_test, tidnet.clone())\n",
    "        best_perf = max(performance.values())\n",
    "        tqdm.tqdm.write(\"Evaluated person {}, Best performance: {:.2%}\".format(test_thinker.person_id, best_perf))\n",
    "\n",
    "        summary = {'Person':test_thinker.person_id,\n",
    "                   \"Before Tuning\": general_process.evaluate(test_thinker)['Accuracy'],\n",
    "                   'Best Result': best_perf}\n",
    "        summary.update(performance)\n",
    "        results.append(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Let's use a `DataFrame` this time to compare the performances a little more effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "results = DataFrame(results)\n",
    "for tune_option in PERFORMANCE_COLUMNS:\n",
    "    results[tune_option + ' Improvement'] = results[tune_option] - results['Before Tuning']\n",
    "print(results.describe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
