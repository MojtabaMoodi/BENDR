{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Basics\n",
    "------\n",
    "This covers a fundamental use-case for DN<sup>3</sup>. Here, the TIDNet architecture from *Kostas & Rudzicz 2020\n",
    "(https://doi.org/10.1088/1741-2552/abb7a7)* is evaluated using the Physionet motor dataset in a\n",
    "*\"leave multiple subjects out\"* (aka person-stratified cross validation) training procedure.\n",
    "This means, the number of subjects are divided into approximately evenly numbered sets, and the test performance of\n",
    "each set is evaluated. The remaining sets for each test will be used to develop a neural network classifier.\n",
    "\n",
    "Here we will focus on classifying imagined hand vs. foot movement, which are the runs 6, 10 and 14. So create a\n",
    "configuration for the configuratron that specifies a single dataset, the length of the trials to cut for the dataset\n",
    " (6s), the events we are looking for (T1 and T2), and then to exclude all those sessions that are not 6, 10 and 14\n",
    "(the sessions by default are listed as \"S{subject number}R{run number}.edf\". Finally, due to some issues with a few\n",
    "people's recordings in the dataset, we can ignore those troublemakers. The following is the contents of a file named\n",
    "\"my_config.yml\".\n",
    "\n",
    "```yaml\n",
    "Configuratron:\n",
    "  preload: True\n",
    "\n",
    "use_gpu: False\n",
    "\n",
    "mmidb:\n",
    "  name: \"Physionet MMIDB\"\n",
    "  toplevel: /path/to/eegmmidb\n",
    "  tmin: 0\n",
    "  tlen: 6\n",
    "  data_max: 0.001\n",
    "  data_min: -0.001\n",
    "  events:\n",
    "    - T1\n",
    "    - T2\n",
    "  exclude_sessions:\n",
    "    - \"*R0[!48].edf\"  # equivalently \"*R0[1235679].edf\"\n",
    "    - \"*R1[!2].edf\"   # equivalently \"*R1[134].edf\"\n",
    "  exclude_people:\n",
    "    - S088\n",
    "    - S090\n",
    "    - S092\n",
    "    - S100\n",
    "  train_params:\n",
    "    epochs: 7\n",
    "    batch_size: 4\n",
    "  lr: 0.0001\n",
    "  folds: 5\n",
    "```\n",
    "\n",
    "Notice that in addition to the dataset and special `Configuratron` section we also include a `train_params` with the\n",
    "dataset.\n",
    "This last part is definitely *not mandatory*, it is an arbitrarily named additional set of configuration values that\n",
    "will be used in our experiment, put there for convenience. See it in action below, but if you don't like it,\n",
    "don't use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from dn3.configuratron import ExperimentConfig\n",
    "from dn3.trainable.processes import StandardClassification\n",
    "from dn3.trainable.models import TIDNet\n",
    "\n",
    "# Since we are doing a lot of loading, this is nice to suppress some tedious information\n",
    "import mne\n",
    "mne.set_log_level(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "First things first, we use `ExperimentConfig`, and the subsequently constructed `DatasetConfig` to rapidly construct\n",
    "our `dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding additional configuration entries: dict_keys(['extensions', 'test_subjects', 'train_params', 'lr'])\n",
      "Configuratron found 1 datasets.\n"
     ]
    }
   ],
   "source": [
    "config_filename = 'my_config_copy.yml'\n",
    "experiment = ExperimentConfig(config_filename)\n",
    "ds_config = experiment.datasets['bci_iv_2a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ds_config.auto_construct_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's create a function that makes a new model for each set of training people and a trainable process for\n",
    "`StandardClassification`. The `cuda` argument handles whether the GPU is used to train the neural network if a\n",
    "cuda-compatible PyTorch installation is operational."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def make_model_and_process():\n",
    "    tidnet = TIDNet.from_dataset(dataset)\n",
    "    return StandardClassification(tidnet, cuda=experiment.use_gpu, learning_rate=ds_config.lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "That's pretty much it! We use a helper that initializes a TIDNet from any `Dataset/Thinker/EpochedRecording` and wrap\n",
    "it with a `StandardClassification` process. Invoking this process will train the classifier.\n",
    "Have a look through all the `trainable.processes`, they can wrap the *same model* to learn in stages (e.g. some sort\n",
    " of fine-tuning procedure from a general model -- covered in `examples/finetuning.ipynb`).\n",
    "\n",
    "Now, we loop through five folds, *leaving multiple subjects out* (`dataset.lmso()`), fit the classifier,\n",
    "then check the test results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars(ds_config.train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for training, validation, test in dataset.lmso(ds_config.folds):\n",
    "    process = make_model_and_process()\n",
    "\n",
    "    process.fit(training_dataset=training, validation_dataset=validation, **ds_config.train_params)\n",
    "\n",
    "    results.append(process.evaluate(test)['Accuracy'])\n",
    "\n",
    "print(results)\n",
    "print(\"Average accuracy: {:.2%}\".format(sum(results)/len(results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Check out how we passed the train_params to `.fit()`, we could specify more arguments for `.fit()` by just adding them\n",
    "to this section in the configuration.\n",
    "\n",
    "Say we wanted to *instead*, get the performance of *each* person in the test fold independently. We could replace the\n",
    "code above with a very simple alternative, that *leaves one subject out* or `.loso()`, that specifically.\n",
    "It would look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "results = dict()\n",
    "for training, validation, test in dataset.lmso(ds_config.folds):\n",
    "    process = make_model_and_process()\n",
    "\n",
    "    process.fit(training_dataset=training, validation_dataset=validation,\n",
    "                epochs=ds_config.train_params.epochs,\n",
    "                batch_size=ds_config.train_params.batch_size)\n",
    "\n",
    "    for _, _, test_thinker in test.loso():\n",
    "        results[test_thinker.person_id] = process.evaluate(test_thinker)\n",
    "\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_acc = sum(v['Accuracy'] for v in results.values()) / len(results)\n",
    "print(\"Average accuracy: {:.2%}\".format(avg_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Try filling in your own values to `my_config.yml` to run these examples on your next read through."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
